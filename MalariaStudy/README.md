# Malaria Study README
## 1. Data Collection
<p>The files for this study were downloaded from the BINP29 course server under /resources/binp29/Data/malaria and saved in the 0_Data folder. The entire analysis was run on the course server, unless stated otherwise.
Then, in a joint effort, the students of the course ran the gene prediction program GeneMark on it. 
In my case, I ran it on the genome of Plasmodium vivax.<p>

```shell
nohup gmes_petap.pl --ES --sequence plasmodium_vivax.genome &
```

<p>This produced a gtf file. The gtf files generated by the different students were collected in a temporary folder on the server and then downloaded into 1_GenePrediction_All, as each took a substantial amount of time to run.<p>

## 2. Processing of Haemoproteus tartakovskyi data
<p>The sequenced genome of Haemoproteus tartakovskyi in scaffolds could also be found on the server. But since malaria is a parasite, the reads do not only contain the parasites dna but also the hosts. We thus needed to filter the reads. This was done by GC content, as Haemoproteus tartakovskyi has a lower GC content (19-42%) compared to the host zebrafinch (41%). Furthermore, all reads shorter than 3000 nucleotides are sorted out as well.<p>

<p> This is done with the python script Scaffold.py.
The minimum scaffold length of 3000 was a given, the GC-content was chosen based on the lecture material. Run with<p>
  
```shell
python Scaffold.py Haemoproteus_tartakovskyi.genome 3000 35 Haemoproteus_genome_filtered.fasta
```
  
<p>The resulting fasta file contains 1681 reads, as opposed to the 2243 of the original. This high percentage is due to how the GC-content border was set. Since it is quite high, it is likely that more bird scaffolds are included but it also guarantees a majority of the Haemoproteus scaffolds to be included.<p>

<p> As a next step the outfile.fasta was also run through GeneMark for a gene prediction.<p>
  
 ```shell
 nohup gmes_petap.pl --ES --sequence Haemoproteus_genome_filtered.fasta & 
 ``` 
  
The resulting gtf file and the fasta produced earlier, together should create a new fasta file, using gffParse.pl (from the BINP29 courseserver). However, first we run into a issue with the headers of the fasta and the headers of the gtf file.                              
```shell
head -1 Haemoproteus.gtf
scaffold01477, GC: 24.2, Length: 3790	GeneMark.hmm	exon	21	1216	0+	.	gene_id "1_g"; transcript_id "1_t";
head -1 Haemoproteus_genome_filtered.fasta
>scaffold00001, GC: 27.63, Length: 116706
```
  
The fasta file recognizes spaces as columnseparators, whereas the gtf file considers tabs columnseperators. It therefore tries to match only the scaffold plus its number with the whole string. The solution is to remove the annotations from the id name in the gtf file. Furthermore, the commas in the fasta file headers are removed.
  
```shell
cat Haemoproteus.gtf | sed "s/,.*\tGeneMark.hmm/\tGeneMark.hmm/1" > Haemoproteus_altered.gtf
cat Haemoproteus_genome_filtered.fasta | tr -d ",">Haemoproteus_genome2_filtered.fasta
```
  
 So that the first line is now.
  
```shell
scaffold01477	GeneMark.hmm	exon	21	1216	0	+	.	gene_id "1_g"; transcript_id "1_t";
```
  
The files produced in the filtering and then geneprediction of Haemoproteus, are saved in 2_Filter_Pred_Haemoproteus. Using them, we can now run the gffParse.pl in a new folder 3_gffParse_Haemoproteus.

```shell
gffParse.pl -c -p -F -i ../2_Filter_Pred_Haemoproteus/Haemoproteus_genome2_filtered.fasta -g ../2_Filter_Pred_Haemoproteus/Haemoproteus_altered.gtf 
```
  
To further exclude scaffolds with bird origin, we use blastp (v. 2.11.0+) and the SwissProt database run in the folder 4_Blast_Haemoproteus.

```shell
blastp -query  ../gffParse_output/gffParse.faa -db SwissProt
```
 
From the output we filter out all the results that have their most significant hit in an avian species. For this we require the information from the NCBI taxonomy file (found on ftp://ftp.ebi.ac.uk/pub/databases/taxonomy/taxonomy.dat) and the Swissprot taxonomy file (ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz). In our case these files are already on the server, so instead we set up a symlink to them.

```shell
ln -s /resources/binp29/Data/malaria/taxonomy.dat taxonomy.dat
ln -s /resources/binp29/Data/malaria/uniprot_sprot.dat uniprot_sprot.dat
```
  
<p> The scaffolds are identified with the script BlastParser.py run in the command line with <p>
  
```shell
python BlastParser.py Blast/Ht_blastout.txt gffParse_output/gffParse.faa genome_files/Haemoproteus_genome2_filtered.fasta taxonomy.dat uniprot_sprot.dat Ht_without_birdscaff.fasta
```
<p>All the files resulting after removing the avian scaffolds were saved in the folder 5_remove_Birds.<p>
<p>Then the geneprediction done earlier for Haemoproteus is run again, because we do not want to predict bird genes. This is done in 6_Pred_Haemoproteus_without_bird.
  
```shell
nohup gmes_petap.pl --ES --sequence ../5_remove_Birds/Ht_without_birdscaff.fasta 
```
<p> The resulting gtf file needs to be also run through gffParse to be used in proteinortho later on.<p>

```shell
#in 6_Pred_Haemoproteus_without_bird needs to be altered again, because of the different head.
cat Ht_pred_without_birds.gtf | sed "s/ GC.*\tGeneMark.hmm/\tGeneMark.hmm/1" > Ht_pred_without_birds_altered.gtf

#in 7_gffParse_All
gffParse.pl -c -p -i ../5_Remove_Birds/Ht_without_birdscaff.fasta -g ../6_Pred_Haemoproteus_without_bird/Ht_pred_without_birds_altered.gtf -b Ht
```
  
<p>To answer a few questions on size of the genomes, gene size and gc contents for all these organisms, we use bash. Please see the project report for the result table. <p>

```shell
#Genome size for all genome files.
for file in *.genome; do (echo $(grep -v "^>" $file | tr -d "\n" | wc -c)); done
#Number of genes for all gtf files.
for file in *.gtf; do cut -f9 $file | cut -d \" -f2 | sort | uniq | wc -l; done
#GC content for all genome files. Results for genome size, and counting G's and C's
for file in *.genome; do grep -v "^>" $file | tr -d "\n" | tr -d "N" | tr -d "A" | tr -d "T"| wc -c; done 
```
  
<p>Next we run the gffParse for all the genomes and their corresponding gtf files in folder 7_gffParse_All.<p>
  
```shell
gffParse.pl -c -p -i ../0_Data/Plasmodium_berghei.genome -g ../1_GenePrediction_All/plasmodium_berghei.gtf -b Pb
gffParse.pl -c -p -i ../0_Data/Plasmodium_cynomolgi.genome -g ../1_GenePrediction_All/plasmodium_cynomolgi.gtf -b Pc
gffParse.pl -c -p -i ../0_Data/Plasmodium_faciparum.genome -g ../1_GenePrediction_All/plasmodium_faciparum.gtf -b Pf
gffParse.pl -c -p -i ../0_Data/Plasmodium_knowlesi.genome -g ../1_GenePrediction_All/knowlesi.gtf -b Pk
gffParse.pl -c -p -i ../0_Data/Plasmodium_vivax.genome -g ../1_GenePrediction_All/plasmodium_vivax.gtf -b Pv
gffParse.pl -c -p -i ../0_Data/Plasmodium_yoelii.genome -g ../1_GenePrediction_All/plasmodium_yoelii.gtf -b Py
gffParse.pl -c -p -i ../0_Data/Toxoplasma_gondii.genome -g ../1_GenePrediction_All/Tg.gff -b Tg
```
<p> proteinortho can be installed over conda. The version we are using is 6.0.33. <p>

```shell
nohup proteinortho6.pl ../7_gffParse_All/{Ht,Pb,Pc,Pf,Pk,Pv,Py,Tg}.faa &
```

<p> Busco can also be installed over conda. Version used is 5.3.0. <p>
```shell
busco -f -i ../7_gffParse_All/Pb.faa -o Pb -m prot -l apicomplexa
#and likewise for the other species.
```
This produces a folder for each species with the outputs. Now we would like to know how many of the BUSCOs are found in all eight organisms. For this we need to do some filehandling. We require one big file containing the unique ids of all BUSCO results for all the eight runs. We only use the BUSCO ids of completed or duplicated BUSCOs. Then we can use uniq -c and grep 8 to retrieve all the results that show up 8 times. 

```shell
#In Pc folder, same for other species, in their corresponding folders. 
cat run_apicomplexa_odb10/full_table.tsv | grep -v "^#" | awk 'BEGIN{FS="\t"}; {if ($2=="Complete" || $2=="Duplicated") print $1 }' | sort | uniq > ../Pc_uniq_id.txt
#Concatenate all the uniq sequences into one file. (In 9_BUSCO folder)
cat *.txt >concatenate_uniq.txt
#To see how many ids are in all 8 files. (188)
cat concatenate_uniq.txt |sort | uniq -c | awk 'BEGINS{FS="\t"}; {if ($1==8) print $2}' | wc -l
#To see how many are in 7. (208)
cat {Pb,Pc,Pf,Pk,Pv,Py,Ht}_uniq_id.txt >concat_without_Tg.txt
cat concat_without_Tg.txt |sort | uniq -c | awk 'BEGINS{FS="\t"}; {if ($1==7) print $2}' | wc -l
```
<p>The Problem with Toxoplasma is, that it has many of its BUSCOs as duplicates. Which we need to keep in mind for the next step. We still want to use it as an outgroup, but to do this we will have to choose which strand to use when having a duplicate in Toxoplasma. <p>

<p> The next step is to take the complete BUSCOs, found in all eight species, and make a fasta file for each, containing the protein sequences corresponding to each species. For this all BUSCOs with COMPLETE are used, except for Toxoplasma, where DUPLICATED is allowed as well. In the case of a duplicate Toxoplasma result, the last one is used. For this step we use BUSCOparser.py. It is executed with <p>
  
```shell
python BUSCOparser.py 8_full_tsv/ faa_files/ output_fastas/
```
<p>Note that the 8_full_tsv directory contains the tsv tables for all 8 species, containing the BUSCO results, the faa_files directory contains the .faa files created with the gffParser earlier for each species, and the output_fastas directory is where   
the output fasta files are stored for each BUSCO. There is a total of 175 BUSCO files. <p>

<p> Having a fasta file for each BUSCO, we can align the sequences in each of them. For this we first run clustalo (v 1.2.4) and then raxml (v 8.2.12). Both are installed over conda. <p>
  
```shell
for file in 10_BUSCO_to_fasta/output_fastas/*.fasta; do new=$(echo ${file##*/}); clustalo -i $file -o 11_Alignments/${new%.fasta}_aligned.faa -v; done
for file in 11_Alignments/*_aligned.faa; do new=$(echo ${file%%_a*}); raxmlHPC -s ${new}_aligned.faa -n ${new##*/}.tre -o Tg -m PROTGAMMABLOSUM62 -p 12345; done
```
  
<p> Having a tree for each protein, we now want to merge them into one tree using phylip consense (v. 3.6.9). Phylip is an older program, it prompts you for input. As input file we use a concatenated file of all tree files created in the previous step. And outgroup is species 8 (Because alphabetically Tg comes last). All the tree files, before and after consensus, are saved in the folder 12_Trees.<p>

```shell
  cat RAxML_result.* > all_trees.tre #make concatenated tree
  phylip consense
 ```
This is the resulting tree:

![](malaria_tree.png)

 Because we are curious, we now concatenate all the Busco protein fastas into one big Busco protein fasta, which still contains one sequence per species, but basically a merged sequence of all the protein sequences. We use Superparser.py for this. Executed in folder 13_SuperTree.

```shell
 python Superparser.py busco_fastas/ big_fasta.faa
```
  
We then proceed to align the sequences with clustalo, and make a tree with raxml.  

```shell
clustalo -i big_fasta.faa -o big_fasta_aligned.faa -v
raxmlHPC -s big_fasta_aligned.faa -n big_fasta.tre -o Tg -m PROTGAMMABLOSUM62 -p 12345
```
This is the resulting tree:
